{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries, versions check and setting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import requests\n",
    "import spacy\n",
    "import spacy_transformers\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from keybert import KeyBERT\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer, KeyphraseTfidfVectorizer\n",
    "from numpy import NaN\n",
    "from os.path import exists as file_exists\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForPreTraining, TrainingArguments, Trainer\n",
    "\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Cuda available\")\n",
    "    cuda_avbl = True\n",
    "    spacy.require_gpu()\n",
    "else:\n",
    "    print(\"Cuda not available\")\n",
    "    cuda_avbl = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE        = \"German\" #{English}                                                                                   # choose language \n",
    "\n",
    "if LANGUAGE     == \"German\":                                                                                            # set model by chosen language\n",
    "    MODEL       = 'dbmdz/german-gpt2'\n",
    "else:\n",
    "    MODEL       = 'gpt2' #{gpt2, gpt2-medium, gpt2-large, gpt2-xl}\n",
    "\n",
    "USE_WIKI        = False                                                                                                 # choose if wiki summary from keywords should used for model training\n",
    "\n",
    "SPECIAL_TOKENS  = {\"bos_token\": \"<|BOS|>\",                                                                              # beginning of a sequenze\n",
    "                   \"eos_token\": \"<|EOS|>\",                                                                              # end of a sequenze\n",
    "                   \"unk_token\": \"<|UNK|>\",                                                                              # set for unknown tokens\n",
    "                   \"pad_token\": \"<|PAD|>\",                                                                              # empty tokens for short sentences\n",
    "                   \"sep_token\": \"<|SEP|>\"}                                                                              # seperates sentences\n",
    "                    \n",
    "MAXLEN          = 1024                                                                                                  # set max token count for gpt2\n",
    "\n",
    "TRAIN_SIZE      = 0.8                                                                                                   # ration for splitting data into training and validation\n",
    "\n",
    "UNFREEZE_LAST_N = 6                                                                                                     # the last N layers to unfreeze for training (6 -> half of all layers)                                                                                                        # set ration of training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Loading and preporcessing Data\n",
    "\n",
    "Load raw descriptions from Excel-file. <br>\n",
    "Extract Keywords from description texts with KeyBert. <br>\n",
    "Save all informations in dictionary. <br>\n",
    " <br>\n",
    "(To save time, complete dictionary can load from json-file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if not file_exists(\"\"):                                                                                             # check if file with all needed data already exists\n",
    "    df = get_raw_data(\"\") #not included in appendix                                                                 # execute function for getting origin data\n",
    "    dic = dict()\n",
    "    for index, row in df.iterrows():\n",
    "        dic[row[\"id\"]] = [row[1], row[2]]                                                                           # create dic with id as key and productname and description as value\n",
    "    data = create_keywords_bert(dic, cuda_avbl)                                                                     # execute function for extracting keywords\n",
    "    \n",
    "    with open(\"\", \"w\") as output:\n",
    "        json.dump(data, output)                                                                                     # save dict with all needed information as json\n",
    "\n",
    "with open('') as input:\n",
    "    data = json.load(input)                                                                                         # load all needed data as dict from json (saves time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only needed, if additional Wikipedia data should be used. <br>\n",
    "<br>\n",
    "All extracted keywords from descriptions are checked for a Wikipedia-page. <br>\n",
    "If one exists, the word and the Wikipedia-summary will be saved as a new entry in the dictionary. <br>\n",
    "The keyowrds are then extracted from the summaries too. <br>\n",
    "<br>\n",
    "(Complete item saved in json-file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if USE_WIKI:\n",
    "    if not file_exists(\"\"):\n",
    "\n",
    "        if not file_exists(\"\"):\n",
    "            import get_wiki_smry\n",
    "\n",
    "        df_wiki = get_raw_data(\"\")\n",
    "        dic_wiki = dict()\n",
    "        for index, row in df_wiki.iterrows():\n",
    "            dic_wiki[row[\"id\"]] = [row[1], row[2]]\n",
    "        data_wiki = create_keywords_bert(dic_wiki, cuda_avbl)\n",
    "        data.update(data_wiki)\n",
    "          \n",
    "        with open(\"\", \"w\") as output:\n",
    "            json.dump(data, output)\n",
    "\n",
    "    with open('') as input:\n",
    "        data = json.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Training Model\n",
    "\n",
    "First, a basic pretrainded model is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = get_tokenier(MODEL, special_tokens=SPECIAL_TOKENS)\n",
    "model = get_model(MODEL, cuda_avbl, tokenizer, special_tokens=SPECIAL_TOKENS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a class for trainings and test Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, tokenizer, randomize=True):\n",
    "\n",
    "        title, text, keywords = [], [], []\n",
    "        for k, v in data.items():\n",
    "            title.append(v[0])\n",
    "            text.append(v[1])\n",
    "            keywords.append(v[2])\n",
    "\n",
    "        self.randomize = randomize\n",
    "        self.tokenizer = tokenizer \n",
    "        self.title     = title\n",
    "        self.text      = text\n",
    "        self.keywords  = keywords  \n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def join_keywords(keywords, randomize=True):\n",
    "        N = len(keywords)\n",
    "\n",
    "        # random sampling and shuffle\n",
    "        if randomize: \n",
    "            M = random.choice(range(N+1))\n",
    "            keywords = keywords[:M]\n",
    "            random.shuffle(keywords)\n",
    "\n",
    "        return ','.join(keywords)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        keywords = self.keywords[i].copy()\n",
    "        kw = self.join_keywords(keywords, self.randomize)\n",
    "        \n",
    "        input = SPECIAL_TOKENS[\"bos_token\"] + self.title[i] + \\\n",
    "                SPECIAL_TOKENS[\"sep_token\"] + kw + SPECIAL_TOKENS[\"sep_token\"] + \\\n",
    "                self.text[i] + SPECIAL_TOKENS[\"eos_token\"]\n",
    "\n",
    "        encodings_dict = tokenizer(input,                                   \n",
    "                                   truncation=True, \n",
    "                                   max_length=MAXLEN, \n",
    "                                   padding=\"max_length\")   \n",
    "        \n",
    "        input_ids = encodings_dict[\"input_ids\"]\n",
    "        attention_mask = encodings_dict[\"attention_mask\"]\n",
    "        \n",
    "        return {\"label\": torch.tensor(input_ids),\n",
    "                \"input_ids\": torch.tensor(input_ids), \n",
    "                \"attention_mask\": torch.tensor(attention_mask)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train and validation dataset in predifined ratio with custom dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = split_data(data, TRAIN_SIZE)                                                                 # excute function for splitting data by defined ration\n",
    "train_dataset = myDataset(train_data, tokenizer)                                                                    # create data sets with created class\n",
    "val_dataset = myDataset(val_data, tokenizer, randomize=False)\n",
    "\n",
    "print(f\"There are {len(train_dataset) :,} samples for training, and {len(val_dataset) :,} samples for validation testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze n layers of the model, so they will not retrained with own data and custom configurations. <br>\n",
    "This reduces training duration and retains general language information gained from pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "for i, m in enumerate(model.transformer.h):        \n",
    "    # only un-freeze the last n transformer blocks\n",
    "    if i+1 > 12 - UNFREEZE_LAST_N:\n",
    "        for parameter in m.parameters():\n",
    "            parameter.requires_grad = True \n",
    "\n",
    "for parameter in model.transformer.ln_f.parameters():        \n",
    "    parameter.requires_grad = True\n",
    "\n",
    "for parameter in model.lm_head.parameters():        \n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set training configurations like epochs, batchsize or learing rate. <br>\n",
    "Training arguments are passed to model trainer. <br>\n",
    "<br>\n",
    "!!! Runtime can last from several hours to days, depending on available hardware !!! <br>\n",
    "If Cuda is available, duration can be reduced to under one hour.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# set arguments for training model\n",
    "if USE_WIKI:                                                                                                        # set path to save model\n",
    "    OUT_PATH    = \"model_with_wiki/\"\n",
    "else:\n",
    "    OUT_PATH    = \"model/\"\n",
    "EPOCHS          = 4\n",
    "TRAIN_BATCHSIZE = 4\n",
    "BATCH_UPDATE    = 16\n",
    "STRATEGY        = \"epoch\" #{\"steps\"}\n",
    "if cuda_avbl:\n",
    "    USE_FP16    = True\n",
    "else:\n",
    "    USE_FP16    = False\n",
    "WARMUP_STEPS    = 1e2\n",
    "LR              = 5e-4\n",
    "EPS             = 1e-8\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUT_PATH,\n",
    "    num_train_epochs=EPOCHS,                                                                                        # number of training epochs\n",
    "    per_device_train_batch_size=TRAIN_BATCHSIZE,                                                                    # batch size per GPU/CPU core\n",
    "    per_device_eval_batch_size=TRAIN_BATCHSIZE,\n",
    "    gradient_accumulation_steps=BATCH_UPDATE,                                                                       # number of steps to accumulate the gradients\n",
    "    evaluation_strategy=STRATEGY,                                                                                   # when model should be evaluated\n",
    "    fp16=USE_FP16,                                                                                                  # using 16-bit precision training or not\n",
    "    fp16_opt_level=\"O1\",\n",
    "    warmup_steps=WARMUP_STEPS,                                                                                      # steps from 0 to learing rate  \n",
    "    learning_rate=LR,                                                                                               # step size at each iteration\n",
    "    adam_epsilon=EPS,                                                                                               # threshold for adaptive learning rates against zero division problems\n",
    "    weight_decay=0.01,                                                                                              # regularization parameter to shrink model weights\n",
    "    disable_tqdm=False,                                                                                             # ensure the display of the progress bar while training\n",
    "    save_strategy=STRATEGY,                                                                                         # when model should be saved     \n",
    "    save_total_limit=1,                                                                                             # maximum number of saved models\n",
    "    load_best_model_at_end=True,     \n",
    ")\n",
    "\n",
    "\n",
    "# define trainer with model, arguments, data, tokenizer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,    \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. Generating Text\n",
    "\n",
    "Loading custom trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenier(MODEL, special_tokens=SPECIAL_TOKENS)\n",
    "model = get_model(MODEL, cuda_avbl, tokenizer, special_tokens=SPECIAL_TOKENS, \n",
    "                load_model_path=r\"model\\pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set product title and additional keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"L'Oréal Age Perfect Rosé Tagescreme\"\n",
    "keywords = [\"Tagescreme\", \"Reinigung\", \"Gesicht\"]\n",
    "kw = myDataset.join_keywords(keywords, randomize=False)                                                             # function for random shuffling keywords\n",
    "\n",
    "prompt = SPECIAL_TOKENS[\"bos_token\"] + title + \\\n",
    "         SPECIAL_TOKENS[\"sep_token\"] + kw + SPECIAL_TOKENS[\"sep_token\"]                                             # start input with beginning token, then insert title and keywords, separated by token\n",
    "         \n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)                                                     # Matrix with encoded input\n",
    "if cuda_avbl:\n",
    "    device = torch.device(\"cuda\")\n",
    "    generated = generated.to(device)\n",
    "\n",
    "model.eval()\n",
    "print(\"Generator ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating multiple sample texts. <br>\n",
    "Beam search: Selects multiple (=int) possible tokens instead of simply the \"best\". <br>\n",
    "After generation more steps, it compares the different possible phrases. These can differ by length and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_outputs = model.generate(inputs=generated,\n",
    "                                max_length=MAXLEN,                                                                  # max lenght of generated text \n",
    "                                min_length=100,                                                                      # min lenght of generated text\n",
    "                                do_sample=True,                                                                     # sampling or always using word with highest probability\n",
    "                                early_stopping=True,                                                                # stopping beamch search when num_beam sentences finished                                                     \n",
    "                                num_beams=5,                                                                        # number of possible tokens that beam search selects        \n",
    "                                temperature=0.9,                                                                    # scales probabilities for a more conservative (lower) or divers (higher) model\n",
    "                                top_k=50,                                                                           # number of most propable tokens to keep                                \n",
    "                                top_p=0.7,                                                                          # keeping only most propable tokens for generation \n",
    "                                repetition_penalty=5.0,                                                             # avoiding sentences that repeat themselves\n",
    "                                num_return_sequences=2                                                              # number of returned descriptions\n",
    "                                )\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    text = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "    a = len(title) + len(','.join(keywords))    \n",
    "    print(\"{}: {}\\n\\n\".format(i+1,  text[a:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8cffa35e1638e1c79492d83b35465f30c828cbf3d6da9ee4a594aeef5efaf2bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
